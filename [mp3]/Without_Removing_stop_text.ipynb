{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15b4b020-5e71-4e5c-812e-edcd7fea7c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders found in the directory:\n",
      "\n",
      "Processing folder: 000\n",
      "\n",
      "Processing folder: 001\n",
      "\n",
      "Processing folder: 002\n",
      "\n",
      "Processing folder: 003\n",
      "\n",
      "Processing folder: 004\n",
      "\n",
      "Processing folder: 005\n",
      "\n",
      "Processing folder: 006\n",
      "\n",
      "Processing folder: 007\n",
      "\n",
      "Processing folder: 008\n",
      "\n",
      "Processing folder: 009\n",
      "\n",
      "Processing folder: 010\n",
      "\n",
      "Processing folder: 011\n",
      "\n",
      "Processing folder: 012\n",
      "\n",
      "Processing folder: 013\n",
      "\n",
      "Processing folder: 014\n",
      "\n",
      "Processing folder: 015\n",
      "\n",
      "Processing folder: 016\n",
      "\n",
      "Processing folder: 017\n",
      "\n",
      "Processing folder: 018\n",
      "\n",
      "Processing folder: 019\n",
      "\n",
      "Processing folder: 020\n",
      "\n",
      "Processing folder: 021\n",
      "\n",
      "Processing folder: 022\n",
      "\n",
      "Processing folder: 023\n",
      "\n",
      "Processing folder: 024\n",
      "\n",
      "Processing folder: 025\n",
      "\n",
      "Processing folder: 026\n",
      "\n",
      "Processing folder: 027\n",
      "\n",
      "Processing folder: 028\n",
      "\n",
      "Processing folder: 029\n",
      "\n",
      "Processing folder: 030\n",
      "\n",
      "Processing folder: 031\n",
      "\n",
      "Processing folder: 032\n",
      "\n",
      "Processing folder: 033\n",
      "\n",
      "Processing folder: 034\n",
      "\n",
      "Processing folder: 035\n",
      "\n",
      "Processing folder: 036\n",
      "\n",
      "Processing folder: 037\n",
      "\n",
      "Processing folder: 038\n",
      "\n",
      "Processing folder: 039\n",
      "\n",
      "Processing folder: 040\n",
      "\n",
      "Processing folder: 041\n",
      "\n",
      "Processing folder: 042\n",
      "\n",
      "Processing folder: 043\n",
      "\n",
      "Processing folder: 044\n",
      "\n",
      "Processing folder: 045\n",
      "\n",
      "Processing folder: 046\n",
      "\n",
      "Processing folder: 047\n",
      "\n",
      "Processing folder: 048\n",
      "\n",
      "Processing folder: 049\n",
      "\n",
      "Processing folder: 050\n",
      "\n",
      "Processing folder: 051\n",
      "\n",
      "Processing folder: 052\n",
      "\n",
      "Processing folder: 053\n",
      "\n",
      "Processing folder: 054\n",
      "\n",
      "Processing folder: 055\n",
      "\n",
      "Processing folder: 056\n",
      "\n",
      "Processing folder: 057\n",
      "\n",
      "Processing folder: 058\n",
      "\n",
      "Processing folder: 059\n",
      "\n",
      "Processing folder: 060\n",
      "\n",
      "Processing folder: 061\n",
      "\n",
      "Processing folder: 062\n",
      "\n",
      "Processing folder: 063\n",
      "\n",
      "Processing folder: 064\n",
      "\n",
      "Processing folder: 065\n",
      "\n",
      "Processing folder: 066\n",
      "\n",
      "Processing folder: 067\n",
      "\n",
      "Processing folder: 068\n",
      "\n",
      "Processing folder: 069\n",
      "\n",
      "Processing folder: 070\n",
      "\n",
      "Processing folder: 071\n",
      "\n",
      "Processing folder: 072\n",
      "\n",
      "Processing folder: 073\n",
      "\n",
      "Processing folder: 074\n",
      "\n",
      "Processing folder: 075\n",
      "\n",
      "Processing folder: 076\n",
      "\n",
      "Processing folder: 077\n",
      "\n",
      "Processing folder: 078\n",
      "\n",
      "Processing folder: 079\n",
      "\n",
      "Processing folder: 080\n",
      "\n",
      "Processing folder: 081\n",
      "\n",
      "Processing folder: 082\n",
      "\n",
      "Processing folder: 083\n",
      "\n",
      "Processing folder: 084\n",
      "\n",
      "Processing folder: 085\n",
      "\n",
      "Processing folder: 086\n",
      "\n",
      "Processing folder: 087\n",
      "\n",
      "Processing folder: 088\n",
      "\n",
      "Processing folder: 089\n",
      "\n",
      "Processing folder: 090\n",
      "\n",
      "Processing folder: 091\n",
      "\n",
      "Processing folder: 092\n",
      "\n",
      "Processing folder: 093\n",
      "\n",
      "Processing folder: 094\n",
      "\n",
      "Processing folder: 095\n",
      "\n",
      "Processing folder: 096\n",
      "\n",
      "Processing folder: 097\n",
      "\n",
      "Processing folder: 098\n",
      "\n",
      "Processing folder: 099\n",
      "\n",
      "Processing folder: 100\n",
      "\n",
      "Processing folder: 101\n",
      "\n",
      "Processing folder: 102\n",
      "\n",
      "Processing folder: 103\n",
      "\n",
      "Processing folder: 104\n",
      "\n",
      "Processing folder: 105\n",
      "\n",
      "Processing folder: 106\n",
      "\n",
      "Processing folder: 107\n",
      "\n",
      "Processing folder: 108\n",
      "\n",
      "Processing folder: 109\n",
      "\n",
      "Processing folder: 110\n",
      "\n",
      "Processing folder: 111\n",
      "\n",
      "Processing folder: 112\n",
      "\n",
      "Processing folder: 113\n",
      "\n",
      "Processing folder: 114\n",
      "\n",
      "Processing folder: 115\n",
      "\n",
      "Processing folder: 116\n",
      "\n",
      "Processing folder: 117\n",
      "\n",
      "Processing folder: 118\n",
      "\n",
      "Processing folder: 119\n",
      "\n",
      "Processing folder: 120\n",
      "\n",
      "Processing folder: 121\n",
      "\n",
      "Processing folder: 122\n",
      "\n",
      "Processing folder: 123\n",
      "\n",
      "Processing folder: 124\n",
      "\n",
      "Processing folder: 125\n",
      "\n",
      "Processing folder: 126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\John Luis Magtoto\\AppData\\Local\\Temp\\ipykernel_14396\\2048420722.py:198: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_set.loc[:, 'Predicted'] = train_set['EmailMessages'].apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.35%\n",
      "Recall: 86.43%\n",
      "Precision: 97.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\John Luis Magtoto\\AppData\\Local\\Temp\\ipykernel_14396\\2048420722.py:203: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_set.loc[:, 'Predicted'] = test_set['EmailMessages'].apply(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import email\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "labels_dataframe = pd.read_csv(\"trec06p-cs280/labels\", sep=\" \", header=None)\n",
    "\n",
    "labels_dataframe.columns = [\"Classification\", \"FileLocation\"]\n",
    "\n",
    "labels_dataframe[\"Classification\"] = labels_dataframe[\"Classification\"].map({\"spam\": 1, \"ham\": 0})\n",
    "\n",
    "labels_dataframe[\"FileLocation\"] = labels_dataframe[\"FileLocation\"].map(lambda x: x.replace(\"../data/\", \"\"))\n",
    "\n",
    "FolderLocation = \"trec06p-cs280/data\"\n",
    "\n",
    "# Initialize DataFrame to store results\n",
    "df = pd.DataFrame(columns=[\"Folder\", \"File\", \"EmailMessages\", \"Classification\"])\n",
    "\n",
    "folders = sorted(os.listdir(FolderLocation))\n",
    "print(\"Folders found in the directory:\")\n",
    "    \n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(FolderLocation, folder)\n",
    "        \n",
    "    # Check if the path is a directory\n",
    "    if os.path.isdir(folder_path):\n",
    "        print(f\"\\nProcessing folder: {folder}\")\n",
    "        files = sorted(os.listdir(folder_path))  # Get the files inside the folder\n",
    "            \n",
    "        for file in files:\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "                \n",
    "            # Read and process each email file\n",
    "            with open(file_path, \"r\", encoding=\"ISO-8859-1\") as email_file:\n",
    "                read_email_file = email_file.read()\n",
    "                parsed = email.message_from_string(read_email_file)  # Parse email\n",
    "\n",
    "                # Function to get the email message from the parsed email\n",
    "                def getMessage(parsed_email):\n",
    "                    if parsed_email.is_multipart():\n",
    "                        return ''.join(part.get_payload(decode=True).decode('ISO-8859-1') \n",
    "                                        for part in parsed_email.walk() \n",
    "                                        if part.get_content_type() == 'text/plain')\n",
    "                    else:\n",
    "                        return parsed_email.get_payload(decode=True).decode('ISO-8859-1')\n",
    "\n",
    "                # Get the email message\n",
    "                msg = getMessage(parsed)  # No cleaning applied to msg\n",
    "\n",
    "                # Get the classification of the email based on the labels DataFrame\n",
    "                # Ensure the file path matches the format in dfLabels\n",
    "                labels_classification = labels_dataframe[labels_dataframe['FileLocation'] == f\"{folder}/{file}\"]['Classification']\n",
    "\n",
    "                # Check if we found a classification\n",
    "                if not labels_classification.empty:\n",
    "                    labels_classification = labels_classification.values[0]\n",
    "                else:\n",
    "                    labels_classification = None  # Default value if not found\n",
    "\n",
    "                # Concatenate the data to the main DataFrame\n",
    "                df = pd.concat([df, pd.DataFrame([[folder, file, msg, labels_classification]], \n",
    "                                                    columns=[\"Folder\", \"File\", \"EmailMessages\", \"Classification\"])], \n",
    "                                ignore_index=True)\n",
    "\n",
    "\n",
    "df['Folder'] = pd.to_numeric(df['Folder'])\n",
    "\n",
    "# Assigning of folder whether they are a training or a test set\n",
    "train_set = df[df[\"Folder\"]<71]\n",
    "test_set = df[df[\"Folder\"]>70]\n",
    "\n",
    "# Train the train set to classify into ham or spam\n",
    "train_ham = train_set[train_set['Classification'] == 0] # Ham\n",
    "train_spam = train_set[train_set['Classification'] == 1] # Spam\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Combine email messages within the train\n",
    "all_messages = pd.concat([train_ham['EmailMessages'], train_spam['EmailMessages']])\n",
    "\n",
    "# Initialize Counters\n",
    "count_ham = Counter()\n",
    "count_spam = Counter()\n",
    "\n",
    "# Count words in ham emails without cleaning\n",
    "count_ham.update(\" \".join(train_ham['EmailMessages']).split())\n",
    "\n",
    "# Count words in spam emails without cleaning\n",
    "count_spam.update(\" \".join(train_spam['EmailMessages']).split())\n",
    "\n",
    "# Combine unique words from both counters\n",
    "unique_words = set(count_ham.keys()).union(set(count_spam.keys()))\n",
    "\n",
    "# Create DataFrame with word counts\n",
    "combined_counts = pd.DataFrame({\n",
    "    'word': list(unique_words),\n",
    "    'ham_count': [count_ham[word] for word in unique_words],\n",
    "    'spam_count': [count_spam[word] for word in unique_words],\n",
    "})\n",
    "\n",
    "# Total Count for each word\n",
    "combined_counts['total'] = combined_counts['ham_count'] + combined_counts['spam_count']\n",
    "\n",
    "# Sort the DataFrame by total count in descending order\n",
    "combined_counts = combined_counts.sort_values(by='total', ascending=False)\n",
    "\n",
    "# Split messages into words and flatten the list\n",
    "all_words = all_messages.str.cat(sep=' ').split()\n",
    "\n",
    "# Count word occurrences\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Get the 10,000 most common words\n",
    "vocabulary = [word for word, _ in word_counts.most_common(10000)]\n",
    "\n",
    "# Create feature matrices for ham and spam training sets\n",
    "def create_feature_matrix(dataframe, vocabulary):\n",
    "    matrix = np.zeros((len(dataframe), len(vocabulary)), dtype=int)\n",
    "    \n",
    "    for i, message in enumerate(dataframe['EmailMessages']):\n",
    "        # Split the message directly without cleaning\n",
    "        message_words = message.split()\n",
    "        for word in message_words:\n",
    "            if word in vocabulary:\n",
    "                matrix[i, vocabulary.index(word)] = 1  # Set 1 for word presence\n",
    "                \n",
    "    return matrix\n",
    "\n",
    "# Create feature matrices\n",
    "ham_feature_matrix = create_feature_matrix(train_ham, vocabulary)\n",
    "spam_feature_matrix = create_feature_matrix(train_spam, vocabulary)\n",
    "\n",
    "# Convert matrices to DataFrames for easier viewing\n",
    "ham_matrix_df = pd.DataFrame(ham_feature_matrix, columns=vocabulary)\n",
    "spam_matrix_df = pd.DataFrame(spam_feature_matrix, columns=vocabulary)\n",
    "\n",
    "ndoc = len(ham_matrix_df) + len(spam_matrix_df)\n",
    "nham = len(ham_matrix_df)\n",
    "nspam = len(spam_matrix_df)\n",
    "\n",
    "P_C_ham = nham / ndoc\n",
    "P_C_spam = nspam / ndoc\n",
    "\n",
    "def laplace_smoothing(feature_matrix_spam, feature_matrix_ham, vocabulary):\n",
    "    # Initialize the probability of each word given spam and ham\n",
    "    word_probabilities_spam = np.zeros(len(vocabulary))\n",
    "    word_probabilities_ham = np.zeros(len(vocabulary))\n",
    "    \n",
    "    # Count occurrences of words in spam and ham emails\n",
    "    word_count_spam = np.sum(feature_matrix_spam, axis=0)\n",
    "    word_count_ham = np.sum(feature_matrix_ham, axis=0)\n",
    "    \n",
    "    # Total number of words in spam and ham\n",
    "    total_words_spam = np.sum(word_count_spam)\n",
    "    total_words_ham = np.sum(word_count_ham)\n",
    "    \n",
    "    # Initialize the Laplace smoothing parameter and the number of classes\n",
    "    smoothing_param = 1\n",
    "    num_classes = 2\n",
    "\n",
    "    # Calculate the likelihood of each word with Laplace smoothing\n",
    "    for i in range(len(vocabulary)):\n",
    "        word_probabilities_spam[i] = (word_count_spam[i] + smoothing_param) / (total_words_spam + smoothing_param * num_classes)\n",
    "        word_probabilities_ham[i] = (word_count_ham[i] + smoothing_param) / (total_words_ham + smoothing_param * num_classes)\n",
    "\n",
    "    return word_probabilities_spam, word_probabilities_ham\n",
    "\n",
    "# Apply Laplace smoothing to calculate the likelihoods of words given spam and ham\n",
    "likelihood_spam, likelihood_ham = laplace_smoothing(spam_feature_matrix, ham_feature_matrix, vocabulary)\n",
    "\n",
    "\n",
    "def classify_email(email, likelihood_ham, likelihood_spam, P_C_ham, P_C_spam, vocabulary):\n",
    "    # Initialize the log probabilities for ham and spam\n",
    "    log_probability_ham = 0\n",
    "    log_probability_spam = 0\n",
    "    \n",
    "    # Split the email into words\n",
    "    words = str(email).split()\n",
    "    \n",
    "    # Compute the log probabilities for each word in the email\n",
    "    for word in words:\n",
    "        if word in vocabulary:  # Check if the word is in the vocabulary\n",
    "            index = vocabulary.index(word)\n",
    "            log_probability_ham += np.log(likelihood_ham[index])\n",
    "            log_probability_spam += np.log(likelihood_spam[index])\n",
    "    \n",
    "    # Add the log probabilities of the prior probabilities for ham and spam\n",
    "    log_probability_ham += np.log(P_C_ham)\n",
    "    log_probability_spam += np.log(P_C_spam)\n",
    "    \n",
    "    # Determine whether it is a spam or ham\n",
    "    return 0 if log_probability_ham > log_probability_spam else 1\n",
    "\n",
    "# Create a new column named predicted and its predicted spam or ham\n",
    "train_set.loc[:, 'Predicted'] = train_set['EmailMessages'].apply(\n",
    "    lambda email: classify_email(email, likelihood_ham, likelihood_spam, P_C_ham, P_C_spam, vocabulary)\n",
    ")\n",
    "\n",
    "# Create a new column named predicted and its predicted spam or ham\n",
    "test_set.loc[:, 'Predicted'] = test_set['EmailMessages'].apply(\n",
    "    lambda email: classify_email(email, likelihood_ham, likelihood_spam, P_C_ham, P_C_spam, vocabulary)\n",
    ")\n",
    "\n",
    "# Defined FP, FN, TP, and TN\n",
    "TP = ((test_set['Classification'] == 1) & (test_set['Predicted'] == 1)).sum()\n",
    "TN = ((test_set['Classification'] == 0) & (test_set['Predicted'] == 0)).sum()\n",
    "FP = ((test_set['Classification'] == 0) & (test_set['Predicted'] == 1)).sum()\n",
    "FN = ((test_set['Classification'] == 1) & (test_set['Predicted'] == 0)).sum()\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "# Calculate Recall\n",
    "recall = TP / (TP + FN)\n",
    "\n",
    "# Calculate Precision\n",
    "precision = TP / (TP + FP)\n",
    "\n",
    "# Display results\n",
    "print(f'Accuracy: {accuracy:.2%}') \n",
    "print(f'Recall: {recall:.2%}')      \n",
    "print(f'Precision: {precision:.2%}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f521ff-bfbd-4f56-a4de-eb90b297f5c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
